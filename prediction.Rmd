---
title: "Practical Machine Learning Prediction Assignment"
author: "Matiki"
date: "October 2, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load necessary R packages into current R session
library(caret)
library(dplyr)
```

## Summary
In this assignment, we will be looking at data collected from 6 participants who
wore accelerometers while performing various exercises. The accelerometers were
worn on the participants' belt, arm, forarm, and dumbbell, while they performed
various lifts correctly and incorrectly in 5 different ways.

We will be building machine learning algorithms to make predictions on the manner
in which the exercises were performed. We will divide the data into training and
test sets, build 4 types of machine learning models on the training data: 
decision tree, random forest, boosting, and linear discirminant analysis. We will
use cross validation to help us select the best performing model, and estimate 
the out-of-sample error rate using the test set. Finally we will apply our best 
performing model to a set of 20 test cases for a final assessment. 

## Getting Data
The first thing to do is to download the data and read it into our current R 
session. The "NA" entries appear in several ways in the data set, and we include 
that information into our function call to read.csv().

```{r}

# Check if data files exist in working directory, if not: download the data
if(!file.exists("pml-training.csv")){
        URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        download.file(URL,
                      destfile = "pml-training.csv")
        rm(URL)
}

if(!file.exists("pml-testing.csv")){
        URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(URL,
                      destfile = "pml-testing.csv")
        rm(URL)
}

# Read the data into R
training <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!",""))
validation <- read.csv("pml-testing.csv", na.strings = c("NA","#DIV/0!",""))

# look at data
# head(training); str(training); summary(training)
```

## Cleaning Data & Preprocessing 
Next we need to split the data into training and test sets. For reproducibility,
we will set the seed, and then split 75% of the data into the training set, the 
rest will become the test set.

```{r}
# split data into train/test sets
set.seed(54321)
inTrain <- createDataPartition(y = training$classe, p = .75, list = FALSE)
trainset <- training[inTrain, ]
testset <- training[-inTrain, ]
rm(inTrain)
rm(training)
```

Next we'll turn the 'classe' variable, which we want to predict, into a factor 
variable, and get rid of the first several columns which are just for 
identification purposes, and won't be needed for our prediction.

```{r}
# coerce 'classe' variable into factor variable
trainset$classe <- as.factor(trainset$classe)
testset$classe <- as.factor(testset$classe)

# remove unnecessary variables related to ID
trainset <- trainset[, -c(1:7)]
```

We won't be able to use our machine learning algorithms if there are NA values,
so we'll need to handle them. We start by checking to see how many NAs we have.

```{r}
# check for NAs
sum(is.na(trainset))
```

Since we have so many NAs, imputation may not be a very effective way of handling 
all the missing values. We'll just remove the variables that have mostly NAs, and 
then check how many NA values are left.

```{r}
# remove rows with greater than 50% NA's
mostlyNA <- sapply(trainset, is.na) %>%
        colMeans() > 0.5
trainset <- trainset[!mostlyNA]
rm(mostlyNA)

# check for NAs again
sum(is.na(trainset))
```

It seems we got rid of all the NA values in our data set. Next we'll check to see
if any variables have close to zero variability. If they do, we might want to 
remove them from our data set before building our models.

```{r}
# check for any variables with nearly zero variance
nzv <- nearZeroVar(trainset, saveMetrics = TRUE)
head(nzv)
sum(nzv$nzv)
rm(nzv)
```

It seems we don't have any variables with near zero variance, so next we'll 
check to see if any of the variables are highly correlated.

```{r}
# check if remaining variables are highly correlated
m <- cor(trainset[,-length(trainset)])
diag(m) <- 0
which(m > 0.8, arr.ind = TRUE)
rm(m)
```

It seems there are several variables with high correlation, so we will account 
for this by adding some principal components analysis to our pre processing step.
We will also do a 5-fold cross validation. Both of these methods are done through
the function call to trainControl() which will be used when we build our models.

```{r}
# do PCA and 5-fold cross validation
tc <- trainControl(method = "cv",
                   number = 5,
                   verboseIter = FALSE , 
                   preProcOptions = "pca",
                   allowParallel = TRUE)
```

## Build the Models
Finally we are ready to build our models. We will use the train() function from
the caret package to build a decision tree, a random forest model, a boosting 
model, and a linear discriminant analysis model.

```{r, cache=TRUE}
# fit a decision tree model
model_tree <- train(classe ~., 
                    data = trainset, 
                    method = "rpart",
                    trControl = tc)

# fit a random forest model
model_rf <- train(classe ~., 
                  data = trainset, 
                  method = "rf",
                  trControl = tc)

# fit boosting model 
model_boost <- train(classe ~., 
                     data = trainset, 
                     method = "gbm",
                     trControl = tc,
                     verbose = FALSE)

# fit model with linear discrimnant analysis
model_lda <- train(classe ~.,
                   data = trainset,
                   method = "lda",
                   trControl = tc)
```

## Evaluate & Compare Model Performance
Now that the models are built, it's time to take a look at how they compare in 
terms of accuracy.

```{r}
# take a look at accuracy of each model
accuracy <- cbind("Decision Tree" = mean(model_tree$results$Accuracy),
                  "Random Forest" = mean(model_rf$results$Accuracy),
                  "GBM Boost" = mean(model_boost$results$Accuracy),
                  "LDA" = mean(model_lda$results$Accuracy))
print(accuracy)
rm(accuracy)
```

It looks like the random forest model produced the greatest accuracy. We will 
use this model on our test set to estimate the out of sample error for our model.

## Evaluate Out-Of-Sample Error

```{r}
# since the random forest model had the best mean performance, we'll use it to 
# make a prediction with our test set.
prediction_rf <- predict(model_rf, newdata = testset)
confusionMatrix(prediction_rf, testset$classe)
```

So it seems our out-of-sample error rate is about 0.63% in our random forest model.

## Predict on Validation Set
Our final step is to use our model to make a prediction on a new set of 20 cases.
This is what will be used for the final assessment of our model and this assignment.

```{r}
# attempt to accurately predict on the validation set
validate_rf <- predict(model_rf, validation)
validate_rf
```
